import{_ as n}from"./plugin-vue_export-helper-c27b6911.js";import{r as t,o as s,c as o,a,b as i,e as r,d}from"./app-cdabc73c.js";const c="/assets/640-1691464522429-111-1323a9da.png",p="/assets/640-1691464522429-112-e88e66d1.png",h="/assets/640-1691464522429-113-b40c0a6b.png",g="/assets/640-1691464522429-114-6a0f0ffb.png",l="/assets/640-1691464522429-115-46ad7beb.png",f="/assets/640-1691464522429-116-50d5691c.png",_="/assets/640-1691464522429-117-ce64a41c.png",u="/assets/640-1691464522429-118-364fc0a0.png",b="/assets/640-1691464522430-119-73cbc48a.png",x="/assets/640-1691464522430-120-49e04c32.png",m="/assets/640-1691464522430-121-560e714c.png",R="/assets/640-1691464522430-122-a00795d7.png",y={},P=a("h1",{id:"_45-白泽带你读论文丨on-training-robust-pdf-malware-classifiers",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#_45-白泽带你读论文丨on-training-robust-pdf-malware-classifiers","aria-hidden":"true"},"#"),i(" 45-白泽带你读论文丨On Training Robust PDF Malware Classifiers")],-1),A=a("p",null,[a("strong",null,"On Training Robust PDF Malware Classifiers")],-1),F={href:"https://arxiv.org/pdf/1904.03542.pdf",target:"_blank",rel:"noopener noreferrer"},D={href:"https://github.com/surrealyz/pdfclassifier",target:"_blank",rel:"noopener noreferrer"},E=d('<p>本文发表在USENIX Security 2020，第一作者是来自哥伦比亚大学的Yizheng Chen.这篇论文的成果来自Suman Jana老师领衔的研究组。该组在机器学习安全领域有很多优秀的成果。</p><h2 id="_1-主要内容" tabindex="-1"><a class="header-anchor" href="#_1-主要内容" aria-hidden="true">#</a> <strong>1. 主要内容</strong></h2><p>尽管基于机器学习的恶意软件分类器性能非常优秀，但一些简单的修改，比如仅增加恶意软件的文件长度，就可以让恶意软件逃避分类器检测，被识别为正常应用。我们把这种通过修改恶意软件逃避检测的攻击称为逃避攻击。</p><p>为了解决自适应攻击者发动的逃避攻击，本篇工作针对PDF恶意软件提出了新的鲁棒训练方法。本篇工作采用Verifiably Robust Training，利用<em>有效PDF必须能被解析为树结构</em>的特点，提出了一种针对PDF树结构的新距离指标，并借助这种距离指标指定了两类鲁棒属性，子树插入和删除。只要攻击者符合鲁棒属性，再强的攻击者也无法产生可逃避分类器检测的变种。比如，指定鲁棒属性为插入1棵子树，任何通过插入1棵子树生成的PDF恶意软件变种都无法逃避检测。</p><p>作者训练了7种鲁棒模型，面对7种不同攻击，与12种基准模型进行了比较。实验结果表明，作者训练的鲁棒模型对3种受鲁棒属性约束的攻击，可以达到92.27％的平均VRA（Verified Robust Accuracy），同时保持99.74％的accuracy和0.56％的FPR，高于已有的模型。即使是未知的自适应攻击者，也需要比基准模型高10倍的L0距离和21倍的PDF基本操作（例如，插入和删除对象）才能逃避作者的鲁棒模型。</p><h2 id="_2-训练目标" tabindex="-1"><a class="header-anchor" href="#_2-训练目标" aria-hidden="true">#</a> <strong>2. 训练目标</strong></h2><p>鲁棒训练的目标，是让<mark>分类器</mark>对逃避攻击有足够的鲁棒性，即对攻击者通过修改恶意软件生成的新恶意变种，分类器也要能将新变种正确分类为恶意软件。具体来说，对于一个输入x，我们首先需要找到对于每一个x˜ ∈ Dk(x)，模型产生的最大损失，称为鲁棒损失。然后<em>以鲁棒损失最小化为目标，更新模型参数</em>。其中Dk(x)指以距离指标D表示的距离k范围内，所有可以产生的与x距离为k的新变种集合。我们将寻找鲁棒损失的过程称为内部最大化问题，将减小鲁棒损失，更新参数的过程称为外部最小化问题。由于外部最小化问题与常规的神经网络训练目标相同，因此鲁棒训练主要问题在于内部最大化问题。</p><figure><img src="'+c+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><h2 id="_3-设计实现" tabindex="-1"><a class="header-anchor" href="#_3-设计实现" aria-hidden="true">#</a> <strong>3. 设计实现</strong></h2><p>这篇工作的设计实现主要由以下4部分构成。</p><h3 id="_3-1-评价指标" tabindex="-1"><a class="header-anchor" href="#_3-1-评价指标" aria-hidden="true">#</a> <strong>3.1 评价指标</strong></h3><p>由于accuracy和FPR不足以评价模型的鲁棒性，作者引入了ERA(Estimated Robust Accuracy)和VRA(Verified Robust Accuracy).</p><p>ERA指对输入x和与其一定距离范围内由已知攻击生成的变种x˜，分类器正确分类的百分比。</p><p>VRA指对输入x和与其一定距离范围内的变种x˜，分类器可以被证明能正确分类的百分比。</p><p>主要的区别在于，ERA以估计描述对已知攻击的鲁棒性，VRA以证明描述对所有攻击的鲁棒性。</p><h3 id="_3-2-特征选择" tabindex="-1"><a class="header-anchor" href="#_3-2-特征选择" aria-hidden="true">#</a> <strong>3.2 特征选择</strong></h3><p>作者使用了前人工作Hidost使用的Bag-of-Path features特征，它将PDF根节点到每个对象的最短结构化路径的二进制计数作为特征。它的好处在于，攻击者修改输入产生新变种的操作不会让特征值产生大的波动，可以有效限制分类器输入的范围，降低FPR.</p><figure><img src="'+p+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><h3 id="_3-3-距离指标和鲁棒属性" tabindex="-1"><a class="header-anchor" href="#_3-3-距离指标和鲁棒属性" aria-hidden="true">#</a> <strong>3.3 距离指标和鲁棒属性</strong></h3><p>为了获取训练所需的鲁棒区域Dk(x)，需要距离指标D描述和限制恶意软件与其变种的相似度。</p><p>作者发现，所有能保留恶意功能的PDF恶意软件变种，都必须满足正确的PDF语法，即可以被解析为树结构。为了能够系统，高效的生成变种，攻击者必然使用子树插入和子树删除操作恶意软件生成变种。只要分类器对这两种操作有鲁棒性，那么它对逃避攻击也就有了鲁棒性。</p><p>作者据此提出了子树距离作为距离指标：两个PDF软件的子树距离，即它们根节点下不同子树的数量。无论在x的根节点下插入1棵怎样的子树，x与生成的变种x˜的子树距离都是1。这样可以更好的限制鲁棒区域，降低FPR.</p><p>借助子树距离的限制，作者指定了子树距离为1时，子树插入和子树删除两种基础鲁棒属性。鲁棒属性具体是：对恶意软件进行任意子树插入（删除）操作生成的子树距离为1的变种，分类器不会将其分类为良性。这些属性都可以推广到子树距离为N的情况。</p><h3 id="_3-4-训练方法" tabindex="-1"><a class="header-anchor" href="#_3-4-训练方法" aria-hidden="true">#</a> <strong>3.4 训练方法</strong></h3><p>解决鲁棒训练内部最大化问题主要有两种方法。过去工作使用的Adversarially Robust Training通过生成具体的对抗变种实例来获取鲁棒损失。这种方法训练的模型只对生成实例采用的攻击者具有鲁棒性，且FPR很高。而本篇工作采用的Verifiably Robust Training方法则通过过度逼近方法，获取鲁棒损失的上界。这种方法对未知攻击也具有一定的鲁棒性，只要给予适当的限制，就能将FPR控制在合理范围内。</p><p>作者在神经网络上使用Symbolic Interval Analysis获取鲁棒损失上界。即通过恶意软件和其变种间特征向量的间隔(interval)来限制输入范围，即Dk(x)，并进行训练。最终获取输出范围，将其上界作为鲁棒损失。</p><h2 id="_4-实验评估" tabindex="-1"><a class="header-anchor" href="#_4-实验评估" aria-hidden="true">#</a> <strong>4. 实验评估</strong></h2><h3 id="数据集" tabindex="-1"><a class="header-anchor" href="#数据集" aria-hidden="true">#</a> 数据集：</h3><p>作者使用的恶意软件数据集来自Contagio，其中划分70%为训练集，30%为测试集。具体情况如下：</p><figure><img src="'+h+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><h3 id="鲁棒属性" tabindex="-1"><a class="header-anchor" href="#鲁棒属性" aria-hidden="true">#</a> 鲁棒属性：</h3><p>实验指定了5个不同的鲁棒属性，分别对应不同子树距离下的子树插入和子树删除。实验时各鲁棒属性生成的间隔数量，以及使用的恶意软件数量如下：</p><figure><img src="'+g+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><h3 id="攻击模型" tabindex="-1"><a class="header-anchor" href="#攻击模型" aria-hidden="true">#</a> 攻击模型：</h3><figure><img src="'+l+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>作者使用了7种攻击模型，各自对应受限自适应攻击（类型Ⅰ），不受限攻击（类型Ⅱ），不受限自适应攻击（类型Ⅲ）三类。它们是否能生成现实恶意软件，评价指标，对分类器的知识以及受鲁棒属性限制情况如下：</p><h3 id="分类模型" tabindex="-1"><a class="header-anchor" href="#分类模型" aria-hidden="true">#</a> 分类模型：</h3><p>作者设计了5类不同的分类模型，第2-5类又有一些不同的分类模型，并对它们的性能进行比较。</p><ol><li>基础神经网络</li><li>对抗再训练</li><li>集成分类器</li><li>单调分类器</li><li>使用本文成果训练的鲁棒分类器</li></ol><h2 id="实验结果" tabindex="-1"><a class="header-anchor" href="#实验结果" aria-hidden="true">#</a> 实验结果：</h2><h3 id="a-受限任意攻击" tabindex="-1"><a class="header-anchor" href="#a-受限任意攻击" aria-hidden="true">#</a> a. 受限任意攻击</h3><p>这类攻击在满足鲁棒属性限制前提下，可以进行任意操作进行逃避攻击。结果如下：</p><figure><img src="'+f+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>可以看出，黑体的两个使用本文成果的模型效果最好。另外，使用某一个鲁棒属性训练的模型，可以获得不同距离下的同类鲁棒属性。比如，鲁棒A模型对鲁棒属性C类型的攻击也有一定的鲁棒性。</p><h3 id="b-梯度攻击" tabindex="-1"><a class="header-anchor" href="#b-梯度攻击" aria-hidden="true">#</a> b. 梯度攻击</h3><p>梯度攻击具有分类模型的所有知识，攻击者沿正向标签的梯度方向持续增加特征值，直到找到可以回避检测的实例。受鲁棒属性约束的梯度攻击结果如下：</p><figure><img src="'+_+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>不受约束时的结果如下：</p><figure><img src="'+u+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>可以看到，受约束时，鲁棒A+B+E的ERA最好。不受约束时，鲁棒模型达到ERA=0时需要的L0距离也大于其它模型。</p><h3 id="c-milp攻击" tabindex="-1"><a class="header-anchor" href="#c-milp攻击" aria-hidden="true">#</a> c. MILP攻击</h3><p>该攻击将逃避问题视为混合整数线性规划(Mixed Integer Linear Program)问题。作者测试了单调分类器对MILP攻击的鲁棒性，结果如下：</p><figure><img src="'+b+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>当L0=15时，单调分类器ERA降为0，但此时鲁棒A+B模型ERA仍有10.54%.</p><h3 id="d-自适应进化攻击" tabindex="-1"><a class="header-anchor" href="#d-自适应进化攻击" aria-hidden="true">#</a> d. 自适应进化攻击</h3><p>使用基因进化算法，并了解模型分类标签和分数。实验结果如下：</p><figure><img src="'+x+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>相比其他模型，攻击者需要3.6倍的L0距离，21倍的操作来逃避鲁棒A+B模型。</p><h3 id="e-逆向模仿攻击" tabindex="-1"><a class="header-anchor" href="#e-逆向模仿攻击" aria-hidden="true">#</a> e. 逆向模仿攻击</h3><p>将恶意负载注入正常PDF，因此不需要了解模型知识，对所有模型策略相同。结果如下：</p><figure><img src="'+m+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>鲁棒A+B+E模型ERA最高。</p><h3 id="f-自适应进化攻击" tabindex="-1"><a class="header-anchor" href="#f-自适应进化攻击" aria-hidden="true">#</a> f. 自适应进化攻击</h3><p>作者结合了Move Exploit攻击和离散攻击，并对比了单调模型和鲁棒模型的性能：</p><figure><img src="'+R+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>相比单调模型，攻击者需要10倍的L0距离和3.7倍的操作来逃避鲁棒A+B+E模型。</p><h2 id="_6-总结评价" tabindex="-1"><a class="header-anchor" href="#_6-总结评价" aria-hidden="true">#</a> <strong>6. 总结评价</strong></h2><p>这篇文章首次提出并训练了PDF恶意软件分类器的鲁棒属性，并取得了不错的效果。由于文中使用的symbolic interval analysis速度很快，这种方法可以推广到更大的神经网络，更大的数据集上。不过实验中使用的一些攻击并不能生成真实环境中的恶意软件，虽然仅存在于特征空间的恶意软件对评价也有一定的意义，但这种方法在真实环境中的FPR可能会略高一些。总的来说，这篇文章对逃避攻击的防御是有不错的启发意义的。</p>',68);function z(k,V){const e=t("ExternalLinkIcon");return s(),o("div",null,[P,A,a("p",null,[i("论文链接："),a("a",F,[i("https://arxiv.org/pdf/1904.03542.pdf"),r(e)])]),a("p",null,[i("开源项目地址："),a("a",D,[i("https://github.com/surrealyz/pdfclassifier"),r(e)])]),E])}const B=n(y,[["render",z],["__file","45-白泽带你读论文丨On Training Robust PDF Malware Classifiers.html.vue"]]);export{B as default};
