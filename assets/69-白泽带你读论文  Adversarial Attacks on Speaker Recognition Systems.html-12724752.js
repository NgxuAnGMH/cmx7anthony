import{_ as i}from"./640-edc8dc9c.js";import{_ as n}from"./plugin-vue_export-helper-c27b6911.js";import{r as o,o as r,c as s,a,b as t,e as p,d as c}from"./app-cdabc73c.js";const g="/assets/640-1691376809617-1-d0c9c176.png",d="/assets/640-1691376809618-2-21c7feba.png",f="/assets/640-1691376809618-3-e672a5b5.png",l="/assets/640-1691376809618-4-7a6c4549.png",S="/assets/640-1691376809618-5-51ffc359.png",_="/assets/640-1691376809618-6-f4ac10ad.png",m="/assets/640-1691376809618-7-b02bd12d.png",h="/assets/640-1691376809618-8-e8340ecc.png",u="/assets/640-1691376809618-9-9f71fcee.png",B="/assets/640-1691376809619-10-73e4f268.png",A="/assets/640-1691376809619-11-4ff40c3f.png",b="/assets/640-1691376809619-12-b0f667f4.png",k="/assets/640-1691376809619-13-08d6f1c6.png",E={},O=a("h1",{id:"_69-白泽带你读论文-adversarial-attacks-on-speaker-recognition-systems",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#_69-白泽带你读论文-adversarial-attacks-on-speaker-recognition-systems","aria-hidden":"true"},"#"),t(" 69-白泽带你读论文 | Adversarial Attacks on Speaker Recognition Systems")],-1),R={href:"https://arxiv.org/abs/1911.01840",target:"_blank",rel:"noopener noreferrer"},y=c('<p><strong>如需转载请注明出处，侵权必究。</strong></p><p>本文发表于S&amp;P 2021，第一作者是来自上海科大的Guangke Chen。</p><p>文章通过提出一种新型对抗攻击方式FAKEBOB，全面系统地评估了在黑盒测试环境下说话人识别系统（Speaker recognition system，SRS）的安全问题。文章中作者使用FAKEBOB在16种不同的攻击场景下进行全面的评估。实验证明，无论面对开源系统还是商业系统，FAKEBOB<em>都能取得非常高的攻击成功率</em>，同时经过测试评估，FAKEBOB生成的对抗样本具备一定迁移能力，并且在真实世界中重放时也能成功完成攻击。作者通过FAKEBOB揭示现有SRS系统存在的安全隐患，并呼吁使用更健壮的防御方式加固SRS系统，从而免受遭遇类似的攻击威胁。</p><h2 id="背景" tabindex="-1"><a class="header-anchor" href="#背景" aria-hidden="true">#</a> 背景</h2><p>日常生活中，说话人识别系统（Speaker recognition system，SRS）在生物认证和身份识别领域被广泛应用，它的流行性也引起安全研究人员的关注。目前针对SRS系统的对抗攻击是在白盒环境下实现的，尚未存在一种攻击方式，可以在黑盒环境下对SRS系统成功实施攻击。因此，这篇文章旨在黑盒条件下，对SRS系统成功实施攻击。</p><h2 id="挑战与整体方案" tabindex="-1"><a class="header-anchor" href="#挑战与整体方案" aria-hidden="true">#</a> 挑战与整体方案</h2><p>具体来说，在黑盒环境下针对SRS实施可靠的对抗攻击存在两个核心挑战：</p><p>1）如何在黑盒环境下设计出不易被察觉的对抗样本生成方案；</p><p>2）如何让对抗样本切实可行，在真实世界依然可以对SRS系统成功实施攻击；</p><p>针对上述挑战，这篇文章首次提出了对于SRS系统的黑盒对抗攻击，称为FAKEBOB。FAKEBOB将对抗样本生成建模为一个带约束(Maximal Distortion)优化问题，同时限制添加的扰动在合理范围内，使人耳不可察。FAKEBOB的主要特点包括：</p><p>1）对三种不同的说话人识别任务，即开集说话人识别（Open-set Identification, OSI）、闭集说话人识别（Close-set Identification, CSI）、SV（Speaker Verification, SV），均可以在黑盒环境下成功实施对抗攻击；</p><p>2）对开源和商用声纹识别系统（talentedsoft）上均取得接近100%攻击成功率，并且能有效迁移到Microsoft Azure声纹识别系统实施对抗攻击。</p><h2 id="设计与实现" tabindex="-1"><a class="header-anchor" href="#设计与实现" aria-hidden="true">#</a> 设计与实现</h2><figure><img src="'+i+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>系统整体设计如图所示，FAKEBOB将生成对抗样本的任务抽象成一个最优化问题（Optimization Problem），即最小化自定义的损失函数（Loss Function），如下：</p><figure><img src="'+g+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>公式中表示损失函数，表示声源。所有输入FAKEBOB的声音都会平坦化至[-1,1]区间，然后再加上一个扰动，得到对抗样本，最后通过一个逆变过程将数据恢复为正常的声音文件格式。对于扰动，使用范数和参数，限制扰动在合理范围内，使这种扰动人耳不可察。</p><p><strong>Attack on OSI</strong>：以开集说话人辨认系统（Open-set Identification，OSI）为例，FAKEBOB的攻击如下图所示，其主要思想是通过迭代的方式，不断调整在攻击者的语音基础上加入的扰动，使得SRS系统会将对抗语音样本识别为来自在系统中注册声纹的某个使用者之一。</p><figure><img src="'+d+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>对于OSI任务，作者设计损失函数如下：</p><p>1）targeted attack</p><figure><img src="'+f+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>2）untargeted attack</p><figure><img src="'+l+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>其中，参数t表示target attack中的攻击目标target，k是攻击强度参数，k值越大则生成的对抗样本攻击能力越强。</p><p>与白盒攻击方式不同，FAKEBOB在黑盒环境下无法知道SRS系统的使用的阈值参数阈值参数，因此本文自己设计了一种阈值参数的估计算法，即通过以一个固定的区间不断调大阈值参数，直到能够生成一个度OSI任务实现untargeted attack的对抗声音样本，将这些样本在SRS系统中的评分的最大值作为预估。算法的伪代码如下：</p><p>同样地，由于黑盒环境无法得知SRS内部具体的损失函数和参数，也就无法直接使用基于梯度的方法来生成对抗样本。为了解决这一问题，FAKEBOB使用NES算法(Natural Evolution Strategy)来计算梯度估计，然后使用BIM（Basic Iterative Method）在估计的梯度方向上不断迭代，得到最优的对抗声音样本。</p><p>对于CSI任务和SV任务，修改了损失函数来生成对应的对抗样本：</p><p><strong>Attack on CSI：</strong></p><p>1）targeted attack</p><figure><img src="'+S+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>2）untargeted attack</p><figure><img src="'+_+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p><strong>Attack on SV</strong>：</p><figure><img src="'+m+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><h2 id="实验评估" tabindex="-1"><a class="header-anchor" href="#实验评估" aria-hidden="true">#</a> 实验评估</h2><p>Effectiveness and Efﬁciency：在通过两种不同方式构建的Kaldi开源SRS系统上，在三种识别任务上FAKEBOB都有接近100%的攻击成功率，此外，FAKEBOB对商用的SRS系统——talentedsoft进行攻击，也取得了100%的攻击成功率，在这个攻击过程中调用API的平均次数为2500次。</p><figure><img src="'+h+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>此外，对于阈值估计算法作者也进行了评估，在十种拥有不同阈值的SRS系统上，该算法能在13.4分钟以内给出略大于正确阈值的估算阈值，证明算法的有效性。</p><figure><img src="'+u+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p><strong>Transferability</strong>：对于Decision-only的SRS系统，可以迁移FAKEBOB生成的对抗样本尝试攻击。在开源系统之间，对于不同参数、不同架构以及不同数据集的SRS系统，FAKEBOB最高可以去的100%的ASR和UTR攻击成功率。此外，将针对SV任务生成的对抗样本迁移到Microsoft Azure上，在针对性攻击（targeted attack）和非针对性攻击（untargeted attack）上攻击迁移成功率分别达到26%和41%。</p><figure><img src="'+B+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p><strong>Practicability for Over-the-Air Attack</strong>：除了通过API投递对抗样本能获得较高的攻击成功率，作者同样在真实环境下对FAKEBOB进行测试。具体来说，将对抗语音经过扬声器播放，经过空气信道传播后，由麦克风接受，在这个过程会因为各种扰动而使对抗样本失效。通过控制变量实验，结果显示FAKEBOB这一方案是有效的，对于不同硬件设备（TABLE VIII）、不同距离（TABLE IX）、不同环境噪音（TABLE X），都能去的较好的攻击成功率。</p><figure><img src="'+A+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><figure><img src="'+b+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><figure><img src="'+k+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p><strong>Human-Imperceptibility via Human Study</strong>：为了衡量加入对抗样本的扰动进行评估，作者在亚马逊MTurk平台上进行了两项问卷调查。第一项调查是询问参与者是否听到语音中存在噪声，第二项调查需要参与者判断听到的一对语音是否来自同一个人。第一项调查结果显示，在Over-the-air环境中可以成功攻击的对抗样本里，有84.8%参与者察觉到了噪声，但这与已知工作中类似的调查结果（83%）相近。第二项调查结果显示，对于包含对抗样本的一对语音中，54%的参数者可以明确分辨出来，但是这对比基线42.2%差距不大。因此总体来说，生成的对比样本跟正常声音没有明显差距。</p><p><strong>Robustness of FAKEBOB against Defense Methods</strong>：最后，论文还验证了FAKEBOB在面对4种语言识别系统防御机制时的攻击效果。对于局部平滑（local smoothing）、比特量化（quantization）、声音挤压（audio squeezing），在增加攻击开销或降低攻击成功率方面效果有限或无效。对于基于时序依赖性的对抗语音检测方法（temporal dependency detection），由于FAKEBOB并没有改变语音文本内容，保留时序依赖性，因此该检测结果接近于随机。</p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结" aria-hidden="true">#</a> 总结</h2><p>本文首次探讨了在黑盒场景下针对SRS系统实现对抗攻击，并在16种攻击场场景下进行全面的评估。无论对开源系统还是商业系统，FAKEBOB都能达到99%升至更高的目标攻击成功率。但是笔者认为FAKEBOB未达到完全黑盒状态，完全黑盒环境应该是类似Microsoft Azure这种完全Decision-only的SRS系统，这也是文章作者后续的优化方向。</p><h2 id="q-a" tabindex="-1"><a class="header-anchor" href="#q-a" aria-hidden="true">#</a> Q&amp;A</h2><p>Q：为什么现有的基于梯度的白盒方法都不能直接用于攻击SRS？作者提出什么方式解决这个问题？</p><p>A：这是由于分数阈值机制，如果预测的分数小于阈值，则会reject，攻击失败。作者提出了一种新的算法来估计阈值，在此基础上，在估计的梯度上使用基本迭代法（BIM）来解决优化问题。</p><p>Q：SRS对于使用者说的话有什么要求，有哪些类别，如何设置会让SRS效果更好以及适用于什么task？</p><p>A：本身有三种：代理模型surrogate model, 梯度估计gradient estimation 和 遗传算法genetic algorithm；现有工作已知梯度估计方法优于代理模型方法，因此把第一种剔除了。后两种作者做了一个对比实验，使用NES梯度估计的方法和PSO遗传算法分别做对SRS进行黑盒攻击，结果显示使用NES梯度估计的方法明显更优。</p><p>Q：现有基于梯度的白盒攻击方式都无法直接应用于我们的攻击场景，这是为什么？</p><p>A：这是由于阈值θ在OSI和SV任务中使用，但在图像/语音识别中没有使用。</p><p>Q：SNR是用于评估什么的指标，是怎么反应好坏的？</p><p>A：在本文中用SNR来衡量攻击样本的失真率，SNR越大则表示失真率越低。</p><p>Q：talentedsoft在使用fakebob会有什么问题？作者建议怎么解决的？</p><p>A：因为talentedsoft是通过HTTP post请求来进行验证即交互的，而fakebob是一种迭代式的方法，频繁发送可能对服务器形成流量压力被察觉，作者建议查询间做一些间隔停顿。</p><p>Q：有哪些可能提议抵御FAKEBOB的方法呢？</p><p>A：1）liveness detection method：伪造的声音需要通过电子设备播放，这种方法通过检查人体发生器官在发声时引入的一些物理特征进行检测；2）用正常样本和对抗样本训练一个模型出来识别：high fp；且当攻击方知道使用了这个防御方式，它就失效了；3）借鉴图片那边的思路，对声音进行输入转换：采用类似bit-depth reduction，MP3 compression的方式。但这些对高置信度的攻击可能会失效；4）speech recognition系统和speaker recognition系统结合，增加攻击难度。</p>',63);function x(F,K){const e=o("ExternalLinkIcon");return r(),s("div",null,[O,a("p",null,[t("["),a("a",R,[t("1911.01840] Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems (arxiv.org)"),p(e)]),t("。")]),y])}const V=n(E,[["render",x],["__file","69-白泽带你读论文  Adversarial Attacks on Speaker Recognition Systems.html.vue"]]);export{V as default};
