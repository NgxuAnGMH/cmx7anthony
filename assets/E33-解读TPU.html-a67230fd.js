import{_ as e}from"./plugin-vue_export-helper-c27b6911.js";import{o as a,c as r,d as m}from"./app-cdabc73c.js";const t="/assets/f6637990792e8de1ef84891fadd11e5e-251a1918.png",p="/assets/6a14254b2bda4dd42adac6a2129e8bae-9f4393d2.jpeg",o="/assets/08e29a700898e5dabf60fbf0f026082a-da5f8489.jpeg",s={},i=m('<h1 id="_33-解读tpu-设计和拆解一块asic芯片" tabindex="-1"><a class="header-anchor" href="#_33-解读tpu-设计和拆解一块asic芯片" aria-hidden="true">#</a> 33 | 解读TPU：设计和拆解一块ASIC芯片</h1><p>过去几年，最知名、最具有实用价值的 ASIC 就是 TPU 了。各种解读 TPU 论文内容的文章网上也很多。不过，这些文章更多地是从机器学习或者 AI 的角度，来讲解 TPU。</p><p>上一讲，我为你讲解了 FPGA 和 ASIC，讲解了 <em>FPGA 如何实现通过“软件”来控制“硬件”</em>，以及我们可以进一步<em>把 FPGA 设计出来的电路变成一块 ASIC 芯片</em>。</p><p>不过呢，这些似乎距离我们真实的应用场景有点儿远。我们怎么能够设计出来一块有真实应用场景的 ASIC 呢？如果要去设计一块 ASIC，我们应该如何思考和拆解问题呢？今天，我就带着你一起学习一下，如何设计一块专用芯片。</p><h2 id="tpu-v1-想要解决什么问题" tabindex="-1"><a class="header-anchor" href="#tpu-v1-想要解决什么问题" aria-hidden="true">#</a> TPU V1 想要解决什么问题？</h2><p>黑格尔说，“世上没有无缘无故的爱，也没有无缘无故的恨”。第一代 TPU 的设计并不是异想天开的创新，而是来自于<em>真实的需求</em>。</p><p>从 2012 年解决计算机视觉问题开始，<em>深度学习一下子进入了大爆发阶段，也一下子带火了 GPU</em>，NVidia 的股价一飞冲天。我们在第 31 讲讲过，GPU 天生适合进行海量、并行的矩阵数值计算，于是它被大量用在深度学习的模型训练上。</p><p>不过你有没有想过，在深度学习热起来之后，计算量最大的是什么呢？并不是进行深度学习的训练，而是深度学习的推断部分。</p><p>所谓<strong>推断部分</strong>，是指我们在完成深度学习训练之后，把训练完成的模型存储下来。这个存储下来的模型，是许许多多个向量组成的参数。然后，我们根据这些参数，去计算输入的数据，最终得到一个计算结果。这个推断过程，可能是在互联网广告领域，去推测某一个用户是否会点击特定的广告；也可能是我们在经过高铁站的时候，扫一下身份证进行一次人脸识别，判断一下是不是你本人。</p><p>虽然训练一个深度学习的模型需要花的时间不少，<em>但是实际在推断上花的时间要更多</em>。比如，我们上面说的高铁，去年（2018 年）一年就有 20 亿人次坐了高铁，这也就意味着至少进行了 20 亿次的人脸识别“推断“工作。</p><p>所以，<em>第一代的 TPU</em>，首先优化的并不是深度学习的模型训练，<em>而是深度学习的模型推断</em>。这个时候你可能要问了，那模型的训练和推断有什么不同呢？主要有三个点。</p><p>**第一点，深度学习的推断工作更简单，对灵活性的要求也就更低。**模型推断的过程，我们只需要去计算一些矩阵的乘法、加法，调用一些 Sigmoid 或者 RELU 这样的激活函数。这样的过程可能需要反复进行很多层，<em>但是也只是这些计算过程的简单组合</em>。</p><p>**第二点，深度学习的推断的性能，首先要保障响应时间的指标。**我们在第 4 讲讲过，计算机关注的性能指标，有响应时间（Response Time）和吞吐率（Throughput）。我们在模型训练的时候，只需要考虑吞吐率问题就行了。因为一个模型训练少则好几分钟，多的话要几个月。而推断过程，像互联网广告的点击预测，我们往往希望能在几十毫秒乃至几毫秒之内就完成，而人脸识别也不希望会超过几秒钟。很显然，<em>模型训练和推断对于性能的要求是截然不同的</em>。</p><p><strong>第三点，深度学习的推断工作，希望在功耗上尽可能少一些</strong>。深度学习的训练，对功耗没有那么敏感，只是希望训练速度能够尽可能快，多费点电就多费点儿了。这是因为，<em>深度学习的推断，要 7×24h 地跑在数据中心里面</em>。而且，对应的芯片，要大规模地部署在数据中心。一块芯片减少 5% 的功耗，就能节省大量的电费。而深度学习的训练工作，大部分情况下只是少部分算法工程师用少量的机器进行。很多时候，只是做小规模的实验，尽快得到结果，节约人力成本。少数几台机器多花的电费，比起算法工程师的工资来说，只能算九牛一毛了。</p><p>这三点的差别，也就带出了第一代 TPU 的设计目标。那就是，<em>在保障响应时间的情况下</em>，能够尽可能地提高<strong>能效比</strong>这个指标，<em>也就是进行同样多数量的推断工作</em>，花费的整体能源要显著低于 CPU 和 GPU。</p><h2 id="深入理解-tpu-v1" tabindex="-1"><a class="header-anchor" href="#深入理解-tpu-v1" aria-hidden="true">#</a> 深入理解 TPU V1</h2><h3 id="快速上线和向前兼容-一个-fpu-的设计" tabindex="-1"><a class="header-anchor" href="#快速上线和向前兼容-一个-fpu-的设计" aria-hidden="true">#</a> 快速上线和向前兼容，一个 FPU 的设计</h3><p>如果你来设计 TPU，除了满足上面的深度学习的推断特性之外，还有什么是你要重点考虑的呢？你可以停下来思考一下，然后再继续往下看。</p><p>不知道你的答案是什么，我的第一反应是，有两件事情必须要考虑，第一个是 TPU <em>要有向前兼容性</em>，第二个是希望 TPU <em>能够尽早上线</em>。我下面说说我考虑这两点的原因。</p><img src="'+t+'" alt="img" style="zoom:25%;"><p>图片来源</p><p>第一代的 TPU 就像一块显卡一样，可以直接插在主板的 PCI-E 口上</p><p>第一点，<em>向前兼容</em>。在计算机产业界里，因为没有考虑向前兼容，惨遭失败的产品数不胜数。典型的有我在第 26 讲提过的安腾处理器。所以，TPU 并没有设计成一个独立的“CPU“，<em>而是设计成一块像显卡一样</em>，插在主板 PCI-E 接口上的板卡。更进一步地，TPU 甚至没有像我们之前说的现代 GPU 一样，设计成自己有对应的取指令的电路，<em>而是通过 CPU</em>，向 TPU 发送需要执行的指令。（<code>没有取指令功能的协处理器</code>）</p><p>这两个设计，使得我们的 TPU <em>的硬件设计变得简单了</em>，我们只需要专心完成一个专用的“<code>计算芯片</code>”就好了。所以，TPU 整个芯片的设计上线时间也就缩短到了 15 个月。不过，这样一个 TPU，其实是第 26 讲里我们提过的 387 浮点数计算芯片，是一个像 FPU（浮点数处理器）的<em>协处理器</em>（Coprocessor），<strong>而不是像 CPU 和 GPU 这样可以独立工作的 Processor Unit</strong>。</p><h3 id="专用电路和大量缓存-适应推断的工作流程" tabindex="-1"><a class="header-anchor" href="#专用电路和大量缓存-适应推断的工作流程" aria-hidden="true">#</a> 专用电路和大量缓存，适应推断的工作流程</h3><p>明确了 TPU 整体的设计思路之后，我们可以来看一看，TPU 内部有哪些芯片和数据处理流程。我在文稿里面，放了 TPU 的模块图和对应的芯片布局图，你可以对照着看一下。</p><img src="'+p+'" alt="img" style="zoom:25%;"><p>图片来源</p><p>模块图：整个 TPU 的硬件，完全是按照深度学习一个层（Layer）的计算流程来设计的</p><p>你可以看到，在芯片模块图里面，有单独的<mark>矩阵乘法单元</mark>（Matrix Multiply Unit）、<mark>累加器</mark>（Accumulators）模块、<mark>激活函数</mark>（Activation）模块和<mark>归一化 / 池化</mark>（Normalization/Pool）模块。而且，这些模块是<em>顺序串联在一起的</em>。</p><p>这是因为，一个深度学习的推断过程，是由很多层的计算组成的。而每一个层（Layer）的计算过程，就是先进行矩阵乘法，再进行累加，接着调用激活函数，最后进行归一化和池化。<em>这里的硬件设计呢，就是把整个流程变成一套固定的硬件电路</em>。这也是一个 ASIC 的典型设计思路，<em>其实就是把确定的程序指令流程，变成固定的硬件电路</em>。</p><p>接着，我们再来看下面的芯片布局图，其中==<strong>控制电路</strong>==（Control）只占了 2%。这是因为，<strong>TPU 的计算过程基本上是一个固定的流程</strong>。不像我们之前讲的 CPU 那样，有各种复杂的控制功能，比如冒险、分支预测等等。</p><p>你可以看到，超过一半的 TPU 的面积，都被用来作为 Local Unified Buffer（<mark><strong>本地统一缓冲区</strong></mark>）（29%）<mark><strong>和矩阵乘法单元</strong></mark>（Matrix Mutliply Unit）了。</p><p>相比于<mark>矩阵乘法单元</mark>和<mark>累加器</mark>、实现<mark>激活函数</mark>和后续的<mark>归一 / 池化</mark>功能的==<strong>激活管线</strong>==（Activation Pipeline）也用得不多。这是因为，在深度学习推断的过程中，矩阵乘法的计算量是最大的，计算也更复杂，所以比简单的累加器和激活函数要占用更多的晶体管。</p><p>而==<strong>统一缓冲区</strong>==（Unified Buffer），则由 SRAM 这样高速的存储设备组成。<strong>SRAM 一般被直接拿来作为 CPU 的寄存器或者高速缓存</strong>。我们在后面的存储器部分会具体讲。SRAM 比起内存使用的 DRAM <strong>速度要快上很多</strong>，<em>但是因为电路密度小，所以占用的空间要大很多</em>。统一缓冲区之所以使用 SRAM，<em>是因为在整个的推断过程中</em>，它会高频反复地被矩阵乘法单元读写，来完成计算。</p><img src="'+o+'" alt="img" style="zoom:20%;"><p>图片来源</p><p>芯片布局图：从尺寸可以看出，统一缓冲区和矩阵乘法单元是 TPU 的核心功能组件</p><p>可以看到，<em>整个 TPU 里面，每一个组件的设计，完全是为了深度学习的推断过程设计出来的</em>。这也是我们设计开发 ASIC 的核心原因：用特制的硬件，最大化特定任务的运行效率。</p><h3 id="细节优化-使用-8-bits-数据" tabindex="-1"><a class="header-anchor" href="#细节优化-使用-8-bits-数据" aria-hidden="true">#</a> 细节优化，使用 8 Bits 数据</h3><p>除了整个 TPU 的模块设计和芯片布局之外，TPU 在各个细节上也充分考虑了自己的应用场景，我们可以拿里面的<mark>矩阵乘法单元</mark>（Matrix Multiply Unit）来作为一个例子。</p><p>如果你仔细一点看的话，会发现这个矩阵乘法单元，没有用 32 Bits 来存放一个浮点数，而是只用了一个 8 Bits 来存放浮点数。这是因为，在实践的机器学习应用中，会对数据做<mark>归一化</mark>（Normalization）和<mark>正则化</mark>（Regularization）的处理。咱们毕竟不是一个机器学习课，所以我就不深入去讲什么是归一化和正则化了，你只需要知道，这两个操作呢，<em>会使得我们在深度学习里面操作的数据都不会变得太大</em>。通常来说呢，都能控制在 -3 到 3 这样一定的范围之内。</p><p><em>因为这个数值上的特征，我们需要的浮点数的精度也不需要太高了</em>。我们在第 16 讲讲解浮点数的时候说过，32 位浮点数的精度，差不多可以到 1/1600 万。如果我们用 8 位或者 16 位表示浮点数，也能把精度放到 2^6 或者 2^12，也就是 1/64 或者 1/4096。<em>在深度学习里，常常够用了</em>。特别是在模型推断的时候，要求的计算精度，往往可以比模型训练低。所以，8 Bits 的矩阵乘法器，就可以放下更多的计算量，<em>使得 TPU 的推断速度更快</em>。</p><h2 id="用数字说话-tpu-的应用效果" tabindex="-1"><a class="header-anchor" href="#用数字说话-tpu-的应用效果" aria-hidden="true">#</a> 用数字说话，TPU 的应用效果</h2><p>那么，综合了这么多优秀设计点的 TPU，实际的使用效果怎么样呢？不管设计得有多好，最后还是要拿效果和数据说话。俗话说，是骡子是马，总要拿出来溜溜啊。</p><p>Google 在 TPU 的论文里面给出了答案。一方面，在性能上，TPU 比现在的 CPU、GPU 在深度学习的推断任务上，<em>要快 15～30 倍</em>。而在能耗比上，<em>更是好出 30～80 倍</em>。另一方面，Google 已经用 TPU 替换了自家数据中心里 95% 的推断任务，<em>可谓是拿自己的实际业务做了一个明证</em>。</p><h2 id="总结延伸" tabindex="-1"><a class="header-anchor" href="#总结延伸" aria-hidden="true">#</a> 总结延伸</h2><p>这一讲，我从第一代 TPU 的设计目标讲起，为你解读了 TPU 的设计。你可以通过这篇文章，回顾我们过去 32 讲提到的各种知识点。</p><p>第一代 TPU，<em>是为了做各种深度学习的推断而设计出来的，并且希望能够尽早上线</em>。这样，Google 才能节约现有数据中心里面的大量计算资源。</p><p>从深度学习的推断角度来考虑，<em>TPU 并不需要太灵活的可编程能力，只要能够迭代完成常见的深度学习推断过程中一层的计算过程就好了</em>。所以，TPU 的硬件构造里面，把矩阵乘法、累加器和激活函数都做成了<mark>对应的专门的电路</mark>。</p><p>为了满足深度学习推断功能的响应时间短的需求，<strong>TPU 设置了很大的使用 SRAM 的 Unified Buffer（UB）</strong>，就好像一个 CPU 里面的寄存器一样，<strong>能够快速响应对于这些数据的反复读取</strong>。</p><p>为了让 TPU 尽可能快地部署在数据中心里面，TPU 采用了现有的 <code>PCI-E 接口</code>，可以和 GPU 一样直接插在主板上，并且采用了作为一个<code>没有取指令功能的协处理器</code>，就像 387 之于 386 一样，<em>仅仅用来进行需要的各种运算</em>。</p><p>在整个电路设计的细节层面，TPU 也尽可能做到了优化。因为机器学习的推断功能，通常做了数值的归一化，所以对于矩阵乘法的计算精度要求有限，<em>整个矩阵乘法的计算模块采用了 8 Bits 来表示浮点数</em>，而不是像 Intel CPU 里那样用上了 32 Bits。</p><p>最终，综合了种种硬件设计点之后的 TPU，<em>做到了在深度学习的推断层面更高的能效比</em>。按照 Google 论文里面给出的官方数据，它可以比 CPU、GPU 快上 15～30 倍，能耗比更是可以高出 30～80 倍。而 TPU，也最终替代了 Google 自己的数据中心里，<em>95% 的深度学习推断任务</em>。</p><h2 id="推荐阅读" tabindex="-1"><a class="header-anchor" href="#推荐阅读" aria-hidden="true">#</a> 推荐阅读</h2><p>既然要深入了解 TPU，自然要读一读关于 TPU 的论文In-Datacenter Performance Analysis of a Tensor Processing Unit。</p><p>除了这篇论文之外，你也可以读一读 Google 官方专门讲解 TPU 构造的博客文章 An in-depth look at Google’s first Tensor Processing Unit(TPU)。</p><h2 id="课后思考" tabindex="-1"><a class="header-anchor" href="#课后思考" aria-hidden="true">#</a> 课后思考</h2><p>你能想一想，如果我们想要做一个能够进行深度学习模型训练的 TPU，我们应该在第一代的 TPU 的设计之上做怎么样的修改呢？</p><p>欢迎留言和我分享你的想法。如果这篇文章对你有收获，你也可以把他分享给你的朋友。</p>',60),n=[i];function P(U,d){return a(),r("div",null,n)}const T=e(s,[["render",P],["__file","E33-解读TPU.html.vue"]]);export{T as default};
