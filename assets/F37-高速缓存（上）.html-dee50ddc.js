import{_ as a}from"./plugin-vue_export-helper-c27b6911.js";import{o as e,c,d as n}from"./app-cdabc73c.js";const s="/assets/4fc459f42a67d3949402865a998bf34b-0c4157e6.png",o="/assets/c1dc0e3453f469fc4607557dab9d5215-23f80dfb.jpg",t="/assets/3a6fcfd1155e03f4f2781dbb6ddaf6cc-4d57b32c.png",r="/assets/522eade51bbfad19fd25eb4f3ce80f22-46eb8833.png",p="/assets/caadd2728b5cfcd2bd704103570f3a80-8bcb8d5d.png",h="/assets/1313fe1e4eb3b5c949284c8b215af8d4-ea6397cd.png",d={},i=n(`<h1 id="_37-高速缓存-上-4毫秒-究竟值多少钱" tabindex="-1"><a class="header-anchor" href="#_37-高速缓存-上-4毫秒-究竟值多少钱" aria-hidden="true">#</a> 37 | 高速缓存（上）：“4毫秒”究竟值多少钱？</h1><p>在这一节内容开始之前，我们先来看一个 3 行的小程序。你可以猜一猜，这个程序里的循环 1 和循环 2，运行所花费的时间会差多少？你可以先思考几分钟，然后再看我下面的解释。</p><div class="language-c line-numbers-mode" data-ext="c"><pre class="language-c"><code><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> arr <span class="token operator">=</span> new <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token number">64</span> <span class="token operator">*</span> <span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">1024</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token comment">// 循环1</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> arr<span class="token punctuation">.</span>length<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*=</span> <span class="token number">3</span><span class="token punctuation">;</span>
<span class="token comment">// 循环2</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> arr<span class="token punctuation">.</span>length<span class="token punctuation">;</span> i <span class="token operator">+=</span> <span class="token number">16</span><span class="token punctuation">)</span> arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*=</span> <span class="token number">3</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在这段 Java 程序中，我们首先构造了一个 <code>64×1024×1024</code> 大小的整型数组。</p><ul><li>在循环 1 里，我们遍历整个数组，将数组中每一项的值变成了原来的 3 倍；</li><li>在循环 2 里，我们每隔 16 个索引访问一个数组元素，将这一项的值变成了原来的 3 倍。</li></ul><p>按道理来说，循环 2 只访问循环 1 中 1/16 的数组元素，只进行了循环 1 中 1/16 的乘法计算，那循环 2 花费的时间应该是循环 1 的 1/16 左右。但是实际上，循环 1 在我的电脑上运行需要 50 毫秒，循环 2 只需要 46 毫秒。<em>这两个循环花费时间之差在 15% 之内</em>。</p><p>为什么会有这 15% 的差异呢？这和我们今天要讲的 CPU Cache 有关。之前我们看到了内存和硬盘之间存在的巨大性能差异。<em>在 CPU 眼里，内存也慢得不行</em>。于是，聪明的工程师们就在 CPU 里面嵌入了 <code>CPU Cache（高速缓存）</code>，来解决这一问题。</p><h2 id="我们为什么需要高速缓存" tabindex="-1"><a class="header-anchor" href="#我们为什么需要高速缓存" aria-hidden="true">#</a> 我们为什么需要高速缓存?</h2><p>按照摩尔定律，CPU 的访问速度每 18 个月便会翻一番，相当于每年增长 60%。内存的访问速度虽然也在不断增长，却远没有这么快，每年只增长 7% 左右。<em>而这两个增长速度的差异</em>，使得 CPU 性能和内存访问性能的差距不断拉大。到今天来看，一次内存的访问，大约需要 120 个 CPU Cycle，这也意味着，在今天，CPU 和内存的访问速度已经有了 120 倍的差距。</p><p>如果拿我们现实生活来打个比方的话，CPU 的速度好比风驰电掣的高铁，每小时 360 公里，然而，它却只能等着旁边腿脚不太灵便的老太太，也就是内存，以每小时 3 公里的速度缓慢步行。<em>因为 CPU 需要执行的指令、需要访问的数据，都在这个速度不到自己 1% 的内存里</em>。</p><img src="`+s+'" alt="img" style="zoom:33%;"><p>随着时间变迁，CPU 和内存之间的性能差距越来越大</p><p>为了弥补两者之间的性能差异，我们能真实地把 CPU 的性能提升用起来，而不是让它在那儿空转，我们在现代 CPU 中引入了<mark>高速缓存</mark>。</p><p>从 <mark>CPU Cache</mark> 被加入到现有的 CPU 里开始，内存中的<mark>指令</mark>、<mark>数据</mark>，会被加载到 <mark>L1-L3 Cache</mark> 中，而不是直接由 CPU 访问内存去拿。<em>在 95% 的情况下，CPU 都只需要访问 L1-L3 Cache，从里面读取指令和数据，而无需访问内存</em>。要注意的是，这里我们说的 CPU Cache 或者 L1/L3 Cache，不是一个单纯的、概念上的缓存（比如之前我们说的拿内存作为硬盘的缓存），而是指<mark>特定的<strong>由 SRAM 组成的</strong>物理芯片</mark>。</p><p>这里是一张 Intel CPU 的放大照片。这里面大片的长方形芯片，就是这个 CPU 使用的 20MB 的 L3 Cache。</p><figure><img src="'+o+'" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>现代 CPU 中大量的空间已经被 SRAM 占据，图中用红色框出的部分就是 CPU 的 L3 Cache 芯片</p><p>在这一讲一开始的程序里，<em>运行程序的时间主要花在了<strong>将对应的数据从<code>内存</code>中读取出来，加载到 <code>CPU Cache</code> 里</strong></em>。CPU 从内存中读取数据到 CPU Cache 的过程中，是一小块一小块来读取数据的，而不是按照单个数组元素来读取数据的。这样一小块一小块的数据，在 CPU Cache 里面，我们把它叫作 <em>Cache Line（缓存块）</em>。</p><p>在我们日常使用的 Intel 服务器或者 PC 里，Cache Line 的大小通常是 <em>64 字节</em>。而在上面的循环 2 里面，我们每隔 16 个整型数计算一次，<em>16 个整型数正好是 64 个字节</em>。于是，循环 1 和循环 2，<strong>需要把<code>同样数量的 Cache Line 数据</code>从内存中读取到 CPU Cache 中，最终两个程序花费的时间<em>就差别不大了</em></strong>。</p><p>知道了为什么需要 CPU Cache，接下来，我们就来看一看，CPU 究竟是如何访问 CPU Cache 的，以及 CPU Cache 是如何组织数据，使得 CPU 可以找到自己想要访问的数据的。因为 Cache 作为“缓存”的意思，在很多别的存储设备里面都会用到。为了避免你混淆，在表示抽象的“缓存“概念时，用中文的“缓存”；如果是 CPU Cache，我会用“高速缓存“或者英文的“Cache”，来表示。</p><h2 id="cache-的数据结构和读取过程是什么样的" tabindex="-1"><a class="header-anchor" href="#cache-的数据结构和读取过程是什么样的" aria-hidden="true">#</a> Cache 的数据结构和读取过程是什么样的？</h2><p><strong>现代 CPU 进行数据读取的时候，无论数据是否已经存储在 Cache 中，<em>CPU 始终会首先访问 Cache</em>。只有当 CPU 在 Cache 中找不到数据的时候，<em>才会去访问内存</em>，并将读取到的数据写入 Cache 之中</strong>。当时间局部性原理起作用后，这个最近刚刚被访问的数据，会很快再次被访问。而 Cache 的访问速度远远快于内存，这样，CPU 花在等待内存访问上的时间就大大变短了。</p><img src="'+t+'" alt="img" style="zoom:50%;"><p>这样的访问机制，和我们自己在开发应用系统的时候，“<em>使用内存作为硬盘的缓存</em>”的逻辑是一样的。在各类基准测试（Benchmark）和实际应用场景中，<code>CPU Cache 的命中率通常能达到 95% 以上</code>。</p><p>问题来了，CPU 如何知道要访问的内存数据，存储在 Cache 的哪个位置呢？接下来，我就从最基本的<strong>直接映射 Cache</strong>（Direct Mapped Cache）说起，带你来看整个 Cache 的数据结构和访问逻辑。</p><h3 id="_1-数据结构" tabindex="-1"><a class="header-anchor" href="#_1-数据结构" aria-hidden="true">#</a> 1 数据结构</h3><p>在开头的 3 行小程序里我说过，CPU 访问内存数据，<em>是一小块一小块数据来读取的</em>。对于读取内存中的数据，我们首先拿到的是数据所在的<strong>内存块</strong>（Block）的地址。而直接映射 Cache 采用的策略，<em>就是确保任何一个<strong>内存块的地址</strong>，始终映射到一个<strong>固定的 CPU Cache 地址</strong>（Cache Line）</em>。而这个映射关系，通常用 <code>mod 运算（求余运算）</code>来实现。下面我举个例子帮你理解一下。</p><p>比如说，我们的主内存被分成 0～31 号这样 32 个块。我们一共有 8 个缓存块。用户想要访问第 21 号内存块。如果 21 号内存块内容在缓存块中的话，它一定在 5 号缓存块（21 mod 8 = 5）中。</p><img src="'+r+'" alt="img" style="zoom:33%;"><p>Cache 采用 mod 的方式，把内存块映射到对应的 CPU Cache 中</p><h3 id="a-低位-索引" tabindex="-1"><a class="header-anchor" href="#a-低位-索引" aria-hidden="true">#</a> A 低位：索引</h3><p>实际计算中，有一个小小的技巧，<em>通常我们会把<strong>缓存块的数量</strong>设置成 <code>2 的 N 次方</code></em>。这样在计算取模的时候，可以直接取地址的<code>低 N 位</code>，也就是<code>二进制里面的后几位</code>。比如这里的 8 个缓存块，就是 2 的 3 次方。那么，在对 21 取模的时候，可以对 21 的 2 进制表示 10101 取地址的<code>低三位</code>，也就是 101，对应的 5，就是<code>对应的缓存块地址</code>。</p><img src="'+p+'" alt="img" style="zoom:33%;"><p>取 Block 地址的低位，就能得到对应的 Cache Line 地址，除了 21 号内存块外，13 号、5 号等很多内存块的数据，都对应着 5 号缓存块中。既然如此，假如现在 CPU 想要读取 21 号内存块，在读取到 5 号缓存块的时候，我们怎么知道里面的数据，究竟是不是 21 号对应的数据呢？同样，建议你借助现有知识，先自己思考一下，然后再看我下面的分析，这样会印象比较深刻。</p><h3 id="b-高位-组标记" tabindex="-1"><a class="header-anchor" href="#b-高位-组标记" aria-hidden="true">#</a> B 高位：组标记</h3><p>这个时候，在对应的缓存块中，我们会存储一个<strong>组标记</strong>（Tag）。这个组标记会记录，<em>当前缓存块内存储的数据对应的内存块</em>，而缓存块本身的地址表示访问地址的<code>低 N 位</code>。就像上面的例子，21 的<code>低 3 位 101</code>，缓存块本身的地址已经涵盖了对应的信息、对应的组标记，我们只需要记录 21 <code>剩余的高 2 位的信息</code>，也就是 10 就可以了。</p><h3 id="补充的有效位" tabindex="-1"><a class="header-anchor" href="#补充的有效位" aria-hidden="true">#</a> ## 补充的有效位</h3><p>除了组标记信息之外，缓存块中还有两个数据。一个自然是从主内存中加载来的实际存放的数据，另一个是<strong>有效位</strong>（valid bit）。啥是有效位呢？它其实就是用来标记，<em>对应的缓存块中的数据是否是有效的，确保不是机器刚刚启动时候的空数据</em>。如果有效位是 0，无论其中的组标记和 Cache Line 里的数据内容是什么，CPU 都不会管这些数据，而要直接访问内存，重新加载数据。</p><h3 id="c-具体字的位置-偏移量" tabindex="-1"><a class="header-anchor" href="#c-具体字的位置-偏移量" aria-hidden="true">#</a> C 具体字的位置：偏移量</h3><p>CPU 在读取数据的时候，<em><code>并不是</code>要读取一整个 Block，<code>而是</code>读取一个他需要的数据片段（Word）</em>。这样的数据，我们叫作 CPU 里的<mark>一个字（Word）</mark>。具体是哪个字，<em>就用这个字<code>在整个 Block 里面的位置</code>来决定</em>。这个位置，我们叫作<mark>偏移量（Offset）</mark>。</p><blockquote><p>组标记Tag（高位）+ 索引Index（低位）= 内存地址基地址（分页分段的头）</p><p>组标记Tag（高位）+ 索引Index（低位）+ 偏移量offset = 完整的内存地址</p><p>内存里的最小存储单位是页大小为4k，这里的分块是cache访问内存的最小单位一块64bit</p></blockquote><h3 id="形象总结图" tabindex="-1"><a class="header-anchor" href="#形象总结图" aria-hidden="true">#</a> 形象总结图</h3><p>总结一下，<strong>一个内存的访问地址，最终包括高位代表的<mark>组标记</mark>、低位代表的<mark>索引</mark>，以及在对应的 Data Block 中定位对应字的<mark>位置偏移量</mark></strong>。</p><img src="'+h+'" alt="img" style="zoom:50%;"><p>内存地址到 Cache Line 的关系</p><h3 id="直接映射-cache-读取过程" tabindex="-1"><a class="header-anchor" href="#直接映射-cache-读取过程" aria-hidden="true">#</a> 直接映射 Cache：读取过程</h3><p>而内存地址对应到 Cache 里的数据结构，则多了一个<mark>有效位</mark>和<mark>对应的数据</mark>，由“<strong>索引 + 有效位</strong> <strong>+ 组标记 + 数据</strong>”组成。如果内存中的数据已经在 CPU Cache 里了，那一个内存地址的访问，就会经历这样 4 个步骤：</p><ol><li>根据内存地址的<strong>低位</strong>，计算在 Cache 中的<mark>索引</mark>；</li><li>判断<mark>有效位</mark>，确认 Cache 中的数据是有效的；</li><li>对比内存访问地址的<strong>高位</strong>，和 Cache 中的<mark>组标记</mark>，确认 Cache 中的数据就是我们要访问的内存数据，从 Cache Line 中读取到对应的<mark>数据块</mark>（Data Block）<em>（一组 Word）</em>；</li><li>根据内存地址的 <mark>Offset 位</mark>，从 Data Block 中，读取希望读取到的*（具体）字 Word*。</li></ol><p>如果在 2、3 这两个步骤中，CPU 发现，Cache 中的数据并不是要访问的内存地址的数据，那 CPU 就会访问内存，并把对应的 Block Data 更新到 Cache Line 中，同时更新对应的有效位和组标记的数据。</p><h3 id="其他-全相连-组相连" tabindex="-1"><a class="header-anchor" href="#其他-全相连-组相连" aria-hidden="true">#</a> ## 其他：全相连 / 组相连</h3><p>好了，讲到这里，相信你明白现代 CPU，是如何通过<code>直接映射 Cache</code>，来定位一个内存访问地址在 Cache 中的位置了。其实，除了直接映射 Cache 之外，我们常见的缓存放置策略还有<code>全相连 Cache</code>（Fully Associative Cache）、<code>组相连 Cache</code>（Set Associative Cache）。这几种策略的数据结构都是相似的，理解了最简单的直接映射 Cache，其他的策略你很容易就能理解了。</p><blockquote><p>2、全相连 Cache（Fully Associative Cache）、<br> 一个快可以放置在cache中的任何位置，但是在检索cache中所有项，为了使检索更加有效，它是由一个与cache中每个项都相关的比较器并行完成，这些比较器加大了硬件开销，因而全相连只适合块较少的cache</p><p>3、组相连 Cache（Set Associative Cache）<br> 介于直接映射和全相连之间的设计是组相连，在组相连cache中，每个块可被放置的位置数是固定的。每个块有n个位置可放的cache被称作为<code>N路组相连cache</code>。一个n路组相联cache由很多个组构成，每个组中有n块，根据索引域，存储器中的每个块对应到cache中唯一组，并且可以放在这个组中的任何一个位置上。</p></blockquote><h2 id="减少-4-毫秒-公司挣了多少钱" tabindex="-1"><a class="header-anchor" href="#减少-4-毫秒-公司挣了多少钱" aria-hidden="true">#</a> 减少 4 毫秒，公司挣了多少钱?</h2><p>刚才我花了很多篇幅，讲了 CPU 和内存之间的性能差异，以及我们如何通过 CPU Cache 来尽可能解决这两者之间的性能鸿沟。你可能要问了，这样做的意义和价值究竟是什么？毕竟，一次内存的访问，只不过需要 100 纳秒而已。1 秒钟时间内，足有 1000 万个 100 纳秒。别着急，我们先来看一个故事。</p><p>2008 年，一家叫作 Spread Networks 的通信公司花费 3 亿美元，做了一个光缆建设项目。目标是建设一条从芝加哥到新泽西，总长 1331 公里的光缆线路。建设这条线路的目的，其实是为了将两地之间原有的网络访问延时，从 17 毫秒降低到 13 毫秒。</p><p>你可能会说，仅仅缩短了 4 毫秒时间啊，却花费 3 个亿，真的值吗？为这 4 毫秒时间买单的，其实是一批高频交易公司。它们以 5 年 1400 万美元的价格，使用这条线路。利用这短短的 4 毫秒的时间优势，这些公司通过高性能的计算机程序，在芝加哥和新泽西两地的交易所进行高频套利，以获得每年以 10 亿美元计的利润。现在你还觉得这个不值得吗？</p><p>其实，<em>只要 350 微秒的差异</em>，就足够高频交易公司用来进行无风险套利了。而 350 微秒，如果用来进行 100 纳秒一次的内存访问，大约只够进行 3500 次。而引入 CPU Cache 之后，我们可以进行的数据访问次数，<em>提升了数十倍</em>，使得各种交易策略成为可能。</p><h2 id="总结延伸" tabindex="-1"><a class="header-anchor" href="#总结延伸" aria-hidden="true">#</a> 总结延伸</h2><p>很多时候，程序的性能瓶颈，来自使用 DRAM 芯片的内存访问速度。</p><p>根据摩尔定律，自上世纪 80 年代以来，CPU 和内存的性能鸿沟越拉越大。于是，现代 CPU 的设计者们，直接在 CPU 中嵌入了使用更高性能的 SRAM 芯片的 Cache，来弥补这一性能差异。通过巧妙地将<em>内存地址</em>，拆分成“<code>索引 + 组标记 + 偏移量</code>”的方式，<em>使得我们可以将很大的内存地址，映射到很小的 CPU Cache 地址里</em>。而 CPU Cache 带来的毫秒乃至微秒级别的性能差异，又能带来巨大的商业利益，十多年前的高频交易行业就是最好的例子。</p><p>在搞清楚从内存加载数据到 Cache，以及从 Cache 里读取到想要的数据之后，我们又要面临一个新的挑战了。CPU 不仅要读数据，还需要写数据，我们不能只把数据写入到 Cache 里面就结束了。下一讲，我们就来仔细讲讲，CPU 要写入数据的时候，怎么既不牺牲性能，又能保证数据的一致性。</p><h2 id="推荐阅读" tabindex="-1"><a class="header-anchor" href="#推荐阅读" aria-hidden="true">#</a> 推荐阅读</h2><p>如果你学有余力，这里有两篇文章推荐给你阅读。</p><p>如果想深入了解 CPU 和内存之间的访问性能，你可以阅读What Every Programmer Should Know About Memory。</p><p>现代 CPU 已经很少使用直接映射 Cache 了，通常用的是组相连 Cache（set associative cache），想要了解组相连 Cache，你可以阅读《<code>计算机组成与设计：硬件 / 软件接口</code>》的 5.4.1 小节。</p><h2 id="课后思考" tabindex="-1"><a class="header-anchor" href="#课后思考" aria-hidden="true">#</a> 课后思考</h2><p>对于二维数组的访问，按行迭代和按列迭代的访问性能是一样的吗？你可以写一个程序测试一下，并思考一下原因。</p><p>欢迎把你思考的结果写在留言区。如果觉得有收获，你也可以把这篇文章分享给你的朋友，和他一起讨论和学习。</p><blockquote><p>分析原因——<br> 1、首选数组的存储方式是连续的，在内存中是按照行来存储的，遍历的时候也是一个一个的往后遍历<br> 2、根据老师讲的，CPU从内存读取数据到CPU Cache ，是按照一小块一小块的方式读取的，它的物理内存是连续的，几乎是同行不同列，如果说我们是按照列来迭代的话，那么就会导致存储快无法使用，我们就不得不从内存中读取数据，而在内存中直接读取数据和从高速缓存中直接读取数据的效率那可不是一般的差距，所以说按照行来迭代话，我们就可以很好的利用的数据块，从高速缓存中来读取数据，从而所花费的时间也就比按照列来迭代所花费的时间少！<br> 作者回复: 你好，理解正确！</p><p>按照行迭代，恰好符合空间局部性。一般二维数组在内存中存放是按行优先存放的 所以在加载数据时候就会把一行数据加载进cache里 这样cache的命中率就大大提高 如果按列迭代 cache就很难名字从而cpu就要经常从内存中读数据 如果数据量不大的话两种方式可能没什么感觉 一旦数据量很大的话按行迭代的速度就能感觉出来了。</p><p>老师 那volatile的作用 是不是就是把cache中的有效位 置为0了呢？<br> 网友回复：这种说法不对，CPU要访问volatile关键字修饰的数据，直接忽略Cache的存在，直接从内存中获取<br> 其他网友：volatile 的作用并没有这么简单。<em>要保证多个CPU对某个地址的变量数据一致</em>，volatile 修饰了w地址，要保证w地址有两个特性：<br> 1、缓存锁定。就是要放置CPU同时对w地址进行修改。<br> 2、多CPU的高速缓存失效。这就是 volatile ，这是一种 CPU 缓存一致性协议实现。<br> 我可能说得不完全对，可以参考一下《Java 并发编程之美》P10页 方腾飞著</p></blockquote>',69),l=[i];function C(m,k){return e(),c("div",null,l)}const b=a(d,[["render",C],["__file","F37-高速缓存（上）.html.vue"]]);export{b as default};
