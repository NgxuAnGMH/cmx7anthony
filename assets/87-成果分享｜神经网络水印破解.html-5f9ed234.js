import{_ as t}from"./plugin-vue_export-helper-c27b6911.js";import{r as n,o,c as l,a as e,b as i,e as r,d as s}from"./app-cdabc73c.js";const p="/assets/640-1689646355116-24-4847c2b6.png",c="/assets/640-1689646355117-25-9175ff2f.png",d="/assets/640-1689646355117-26-1e3d18e9.png",h="/assets/640-1689646355117-27-899be01a.jpeg",g="/assets/640-1689646355117-28-57a85a1f.png",u="/assets/640-1689646355117-29-f5aa03b3.png",m="/assets/640-1689646355117-30-a8c81feb.png",f={},_=e("h1",{id:"_87-成果分享-神经网络水印破解",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_87-成果分享-神经网络水印破解","aria-hidden":"true"},"#"),i(" 87-成果分享｜神经网络水印破解")],-1),b={href:"https://www.usenix.org/conference/usenixsecurity23/presentation/yan",target:"_blank",rel:"noopener noreferrer"},k=s('<p>今天分享我实验室的白泽智能团队（Whizard AI）的最新研究Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation，该工作发现了9种发表在包括TPAMI、ICML、CVPR、ASPLOS等AI与系统顶会上的主流神经网络白盒水印技术存在普遍漏洞，提出<mark>神经网络混淆技术</mark>，能够在模型性能完全无损条件下实现水印完全消除，目前该研究已被USENIX Security 2023录用。</p><figure><img src="'+p+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><h2 id="dnn模型水印" tabindex="-1"><a class="header-anchor" href="#dnn模型水印" aria-hidden="true">#</a> DNN模型水印</h2><p>随着深度神经网络（DNN）的训练成本不断上升，由AI企业提供的预训练模型正面临着<em>被攻击和盗用</em>的威胁。攻击者可利用公开发布的预训练模型窃取敏感的隐私数据、盗用他人的学术成果或进行非法牟利，对DNN模型的所有权保护提出了挑战。</p><p>为了对网络上的可疑模型进行溯源和取证，<mark>DNN模型的水印机制</mark>在近年成为广受关注的话题。通过水印的嵌入和提取算法，模型所有者可在模型中嵌入含有版权信息的水印标识，并在发布后对可疑模型进行水印的提取和验证，以检测该模型是否为窃取所得。随着模型水印的可用性、鲁棒性和隐蔽性不断得到提升，这一机制已成为模型所有权保护的主流方法。</p><p>根据验证阶段对可疑模型的访问权限，现有的DNN模型水印可分为<mark>白盒水印</mark>与<mark>黑盒水印</mark>两种。</p><ol><li>黑盒水印通过构建特殊的输入-输出对，将水印信息植入模型的预测行为；</li><li>而在白盒场景下，模型所有者可在模型参数、神经元激活值等内部特征上嵌入水印，不仅为版权验证提供了更多信息，也能更好地保持原模型的性能，因此近年的诸多工作都聚焦于对白盒模型水印的优化。</li></ol><h3 id="白盒水印-黑盒水印" tabindex="-1"><a class="header-anchor" href="#白盒水印-黑盒水印" aria-hidden="true">#</a> ###白盒水印/黑盒水印？</h3><blockquote><p>AI模型的&quot;白盒水印&quot;和&quot;黑盒水印&quot;是两种常见的用于保护和追踪模型的方法，它们具有不同的作用和实现方式。</p><ol><li><p>白盒水印（White-box Watermarking）：白盒水印是将可追溯的标记嵌入到AI模型的内部结构中的一种技术。白盒水印对模型的结构进行了修改，通常通过更改模型的权重、参数、激活函数等方式进行嵌入。白盒水印的主要作用是防止模型被未经授权的使用或盗用，以及保护模型所有者的知识产权。如果未经授权的用户尝试使用带有白盒水印的模型，水印会被检测到，并可用于追踪侵权行为。</p></li><li><p>黑盒水印（Black-box Watermarking）：黑盒水印是一种在不改变模型内部结构的情况下，通过对输入数据进行增加或修改来嵌入水印的方法。黑盒水印通常会修改输入数据的特征或属性，而对于模型本身而言是透明的。黑盒水印的目的是保护数据的来源和知识产权。当模型的输出结果出现侵权行为时，可以通过检测水印特征来追踪数据的来源。</p></li></ol><p>这两种水印的实现方式和差异有以下几点：</p><ol><li><p>实现方式：白盒水印需要对模型的内部结构进行修改，通常需要访问和修改模型的权重、参数等；而黑盒水印则是在输入数据层面进行水印嵌入，不需要对模型本身进行修改。</p></li><li><p>水印类型：白盒水印是一种可追溯的标记，可以被检测和追踪，用于防止未授权的使用和盗用；而黑盒水印是一种保护数据来源和知识产权的标记，用于追踪数据的流向和使用情况。</p></li><li><p>对模型性能的影响：白盒水印可能会对模型的性能有一定的影响，因为它需要修改模型的结构和参数；而黑盒水印对模型的性能没有直接的影响，因为它主要是在输入数据层面进行操作。</p></li></ol><p>无论是白盒水印还是黑盒水印，它们的主要目的是保护AI模型的知识产权和数据来源，防止模型被滥用和盗用。根据具体的应用场景和需求，选择合适的水印方案来保护AI模型的安全性和可信度。</p></blockquote><h2 id="现有攻击的局限性" tabindex="-1"><a class="header-anchor" href="#现有攻击的局限性" aria-hidden="true">#</a> 现有攻击的局限性</h2><p>为了增强白盒模型水印的鲁棒性，许多工作也提出了可能的攻击策略。在获取含有水印的预训练模型后，攻击者可通过修改模型结构或内部参数，来破坏模型中的水印信息，并希望其攻击行为不影响模型的原有性能，且具有较低的计算开销。通过对现有攻击方法的分析（如论文中Table 1所示），作者们发现它们并未同时满足可用性和实用性的要求。部分修改模型参数的方法（如剪枝）必然会导致模型的性能受损，而修改模型结构的方法（如模型提取）则引入了较高的训练成本，甚至需要对目标数据集和水印机制的额外知识，不符合现实的攻击条件。</p><figure><img src="'+c+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><h2 id="神经元结构混淆攻击" tabindex="-1"><a class="header-anchor" href="#神经元结构混淆攻击" aria-hidden="true">#</a> 神经元结构混淆攻击</h2><p>通过对现有白盒水印机制的分析，这篇论文指出了它们的共有问题，即假设<em>攻击者窃取的模型</em>和<em>原模型</em>具有相同的神经元结构。基于这一弱点，作者们设计了一种新的攻击方法——<mark>神经元结构混淆攻击</mark>，通过插入一组不影响模型正常行为的伪造神经元（dummy neurons），可在无需额外知识和性能损失的前提下，<strong>严重阻碍水印信息的正确提取</strong>。为了提升攻击的隐蔽性，他们还设计了进一步的混淆策略，使防御者难以检测和消除模型中的伪造神经元。整体的攻击流程如论文中Fig. 2所示。</p><figure><img src="'+d+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>在伪造神经元的生成阶段，攻击者的目标是在阻碍水印验证的同时，保持窃取模型的性能不受影响。这篇论文提出了两种攻击策略，分别从<strong>添加和拆分神经元</strong>的角度，来破坏模型的原有结构。</p><p>假设任一神经元与前一层网络间的参数为其输入参数（incoming weights），与后一层网络间的参数为其输出参数（outgoing weights），本文设计的NeuronClique方法通过<mark>插入</mark><em>一组输入参数相同、输出参数之和为0的神经元</em>，来抵消它们对该层输出结果的影响（如论文中Fig. 3所示）；而NeuronSplit方法则对某个神经元进行<mark>拆分</mark>，<em>将其替换为功能不变的一组神经元</em>，并保持伪造神经元的参数分布不变（如原文中Fig. 4所示）。通过这两种生成方法，攻击者能以从后向前的顺序，在模型中逐层插入伪造神经元，这一过程只需很低的计算成本，且不会改变模型的输入输出维度，具有较高的攻击效率。</p><figure><img src="'+h+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>为了避免插入的虚假神经元被检测和移除，攻击者还可以采用进一步的混淆策略，来提升攻击的隐蔽性。</p><ol><li>一方面，作者利用DNN对神经元顺序变化和参数缩放的不变性，提出可对伪造神经元的输入、输出参数进行同比例的缩放，或对伪造神经元的排列顺序进行随机置换，以降低防御检测的成功率；</li><li>另一方面，也可在插入伪造神经元的同时，对卷积核的参数矩阵进行填充，进一步扰乱模型水印的验证过程。</li></ol><h2 id="评估-攻击效果" tabindex="-1"><a class="header-anchor" href="#评估-攻击效果" aria-hidden="true">#</a> 评估：攻击效果</h2><p>这篇论文在现有的9种白盒水印机制上都进行了攻击效果的评估，以攻击后<em>水印提取的错误率</em>（Bit Error Rate, BER）来衡量攻击的有效性。为了确保验证阶段的正常进行，他们为各水印方法设计了<em>错误处理机制</em>（Error-handling Mechanisms），并选取绝对值最大的部分参数用于水印提取。</p><p>论文中的Fig. 5展示了在不同攻击强度下的实验效果。在本文提出的攻击方法下，大部分水印机制的BER都超过了50%，即成功消除了原模型中的水印信息。<strong>对于其中多种水印机制，只需加入5%数量的伪造神经元，即可破坏水印的验证过程，反映了目前水印方法设计的共有缺陷</strong>。</p><figure><img src="'+g+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><h2 id="评估-攻击隐蔽性" tabindex="-1"><a class="header-anchor" href="#评估-攻击隐蔽性" aria-hidden="true">#</a> 评估：攻击隐蔽性</h2><p>最后，为了验证本文攻击方法的隐蔽性，作者尝试在多种场景下检测并消除伪造神经元。</p><p>当防御者已知模型中存在伪造神经元时，可采用基于聚类（cluster-based）或基于奇异值分解（SVD-based）的方法，根据神经元的输入、输出参数来区分正常神经元和伪造神经元。此类检测方法的效果如论文中Fig. 7所示，<strong>由于NeuronSplit等方法生成的伪造神经元与正常神经元具有相同的参数分布，因此可成功绕过此类方法的检测</strong>。</p><h3 id="聚类-奇异值分解" tabindex="-1"><a class="header-anchor" href="#聚类-奇异值分解" aria-hidden="true">#</a> ###聚类/奇异值分解？</h3><blockquote><p>聚类方法和奇异值分解（SVD）方法是数据分析和机器学习中常用的两种技术，它们的概念和应用有些异同。</p><ol><li><p>聚类方法：<br> 聚类方法是一种将数据集中的对象根据其相似性进行分组或聚类的技术。它的目标是将数据集中的对象划分为不同的群集或簇，使得同一群集内的对象相似度较高，而不同群集之间的相似度较低。聚类方法根据对象之间的距离或相似度进行分组，并通过调整聚类算法的参数或距离度量来优化聚类结果。聚类方法在数据挖掘、模式识别等领域广泛应用，可以帮助发现数据中的自然结构和类别。</p></li><li><p>奇异值分解方法（Singular Value Decomposition，SVD）：<br> 奇异值分解是一种将一个矩阵分解为三个矩阵的过程，使得矩阵的重构误差最小化。具体来说，给定一个矩阵，SVD将其分解为三个矩阵的乘积：一个正交矩阵U、一个对角矩阵Σ和另一个正交矩阵V的转置。其中，矩阵Σ的对角元素称为矩阵的奇异值。奇异值分解在数据降维、矩阵压缩、信号处理等领域广泛用于特征提取和信息提取。</p></li></ol><p>异同点：</p><ul><li>聚类方法和奇异值分解方法都是用于处理数据的。</li><li>聚类方法通过将数据对象分组为不同的簇或群集来发现数据中的结构和类别。</li><li>奇异值分解方法通过将矩阵分解为三个矩阵的乘积来提取特征或信息。</li><li>聚类方法主要用于无监督学习，即不需要预先标记的数据。而奇异值分解方法可以用于无监督和监督学习。</li><li>聚类方法根据对象之间的相似性进行分组，而奇异值分解方法通过分解矩阵来提取特征。</li><li>聚类方法的输出是数据对象的分组或群集，而奇异值分解方法的输出是分解后的矩阵的三个因子。</li></ul><p>总的来说，聚类方法主要用于发现数据的自然结构和类别，而奇异值分解方法主要用于降维、特征提取和信息提取。它们在处理数据时有着不同的目的和应用领域。</p></blockquote><figure><img src="'+u+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>当防御者已知本文的攻击算法时，可对每层中的神经元参数进行归一化，并将归一化后参数相同的神经元进行合并，再根据其输出参数的特点进行针对性的消除。由论文中Table 3的实验结果可见，<em>由于防御者无法获取原水印模型，虽然此类检测能消除攻击者插入的伪造神经元，但无法恢复其他神经元的原始参数，因此仍无法提取出正确的水印信息，可见本文的攻击方法具有较强的隐蔽性</em>。</p><figure><img src="'+m+'" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><h2 id="结语" tabindex="-1"><a class="header-anchor" href="#结语" aria-hidden="true">#</a> 结语</h2><p>在这篇论文中，作者聚焦于DNN模型的白盒水印机制，分析了现有攻击方法在现实场景下的局限性，并利用白盒水印方法的共有弱点，提出了对神经元结构进行混淆的攻击方法。作者分享了自动生成与注入伪造神经元的攻击流程，并在9种主流模型白盒水印机制上验证了攻击的有效性和隐蔽性。</p><p>团队简介</p><p>白泽智能团队负责人为张谧教授，隶属于杨珉教授领衔的复旦大学系统软件与安全实验室的白泽智能团队。该团队主要研究方向为AI系统安全，包括AI供应链安全、数据隐私与模型保护、模型测试与优化、AI赋能安全等研究方向，在S&amp;P、USENIX Security、CCS、TPAMI、ICML、NeurIPS、KDD等网络安全和AI领域国际顶会顶刊已发表论文30余篇。</p>',36),x={href:"https://mi-zhang-fdu.github.io/index.chn.html",target:"_blank",rel:"noopener noreferrer"},N={href:"https://whitzard-ai.github.io/",target:"_blank",rel:"noopener noreferrer"};function I(S,D){const a=n("ExternalLinkIcon");return o(),l("div",null,[_,e("p",null,[e("a",b,[i("Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation | USENIX"),r(a)]),i("。")]),k,e("p",null,[i("张谧教授个人主页："),e("a",x,[i("https://mi-zhang-fdu.github.io/index.chn.html"),r(a)])]),e("p",null,[i("白泽智能团队（Whizard AI）："),e("a",N,[i("https://whitzard-ai.github.io/"),r(a)])])])}const y=t(f,[["render",I],["__file","87-成果分享｜神经网络水印破解.html.vue"]]);export{y as default};
